# --- Model configuration ----------------------------------------------------
# Baseline general-purpose chat model for drafting & explanation
GENERAL_MODEL=llama3.1:8b

# Reasoning mode model for slower, deeper analysis
REASONING_MODEL=deepseek-r1:8b

# Embedding model for retrieval (must be available in Ollama)
EMBED_MODEL=llama3.1:8b

# --- Retrieval configuration -------------------------------------------------
# Maximum characters per text chunk before embedding/storing
MAX_CHARS_PER_CHUNK=1500

# How many top chunks to use when building evidence context
TOP_K_CHUNKS=12

# --- Generation controls -----------------------------------------------------
# Conservative defaults for assessment / evidence work
TEMPERATURE=0.2
TOP_P=0.9
MAX_TOKENS=1024

# Prompt behavior versioning for auditability
PROMPT_VERSION=bench-rag-v1

# --- Service configuration ---------------------------------------------------
# Where chat-api reaches Ollama (container-to-container URL)
OLLAMA_HOST=http://ollama:11434
